---
title: "RiskML: Mapping and Quantifying ML Failures"
image: "/uploads/avid.png"
layout: page
---

## About
Text-to-image (TTI) generative AI (such as Stable Diffusion, DALL-E, MidJourney) inherits many of the risks that come with large-scale models that can be adapted for a wide variety of downstream tasksâ€”like algorithmic monoculture and the difficulty of anticipating use cases. Yet relative to the enormous scrutiny received by the failures of large language models, shared knowledge of the novel harms and risks of TTI models. 

How can we build boundary objects that support meaningful collaboration between researchers, impacted communities, and practitioners in mitigating the harms of models that pose novel risks? Our hands-on workshop tackles this issue through a strategy inspired by model documentation and cybersecurity best practices (NIST CVE, MITRE ATT&CK). Our aim is to combine practical experience with the potential and limitations of TTI models, building open resources that minimize harmful misuses of ML, and supporting cross-disciplinary efforts to make open-source models and datasets less harmful to impacted communities.

## Schedule
This workshop is proposed to be held at NeurIPS, New Orleans, USA during December 2023.

## Organizers
- [Jekaterina Novikova](https://jeknov.github.io/), Cambridge
Cognition/Winterlight Labs
- [Borhane-Blili Hamelin](https://borhane.xyz/), BABL AI
- [Carol Anderson](https://www.linkedin.com/in/carolmanderson/), AI Risk and Vulnerability Alliance
- [Leon Derczynski](https://t.co/cSxb6GWRwt), University of Washington, IT University of Copenhagen
- [Subho Majumdar](subhomajumdar.com), Amazon