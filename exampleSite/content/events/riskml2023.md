---
title: "RiskML: Mapping and Quantifying ML Failures"
image: "/uploads/avid.png"
layout: page
---

## About
We aim to establish a shared knowledge base of machine learning (ML) risks and failures by collaboratively mapping them to the open-source AI Vulnerability Database (AVID). The absence of answers to a number of open questions related to failure modes in ML pipelines pose a barrier to advancing model development, deployment, and adoption in real-life. Addressing this need, our workshop intends to cover the current landscape of ML risks and failure modes across multiple domains and deployment stages by bringing together diverse expert communities, fostering nascent research, and seeding the practice of structure incident reporting in a ML context. We plan to achieve this through a slate of diverse activities, including a dual submission track of research papers and collaborative tasks, a panel discussion, breakout sessions, and oral and poster presentations.

## Information
This workshop is proposed to be held at NeurIPS, New Orleans, USA during December 2023.

The RiskML workshop has the following core goals:
- Bring together diverse expert communities to discuss challenging questions relating to risks, vulnerabilities, and failures of ML models and datasets.
- Establish a shared state-of-the-art knowledge base of failure modes for ML models, datasets, and systems that is created not by one group, but by the broader community.

To reach these goals, we welcome two different types of submissions: _regular workshop submissions_ and _collaborative task submissions_. The latter will consist of a vulnerability report artefact and a companion paper motivating and evaluating the submission. In both cases, we accept archival papers and extended abstracts.

### Position papers
Towards our first goal, we invite paper submissions on topics related to risks and vulnerabilities in ML. Such submissions present work on the topic of the workshop (see examples listed below), but are not intended to be included on the AVID knowledge base. Regular workshop papers may be submitted as an archival paper, when they report on completed, original and unpublished research; or as a shorter extended abstract. 

Topics of interest include, but are not limited to:

- Position papers on taxonomies of ML risks, harms, vulnerabilities and failures;
- Empirical studies that propose new paradigms to evaluate risks and vulnerabilities of ML models;
- Meta analyses comparing how different existing risk taxonomies are related or different;
- Papers that discuss why, when and how evaluating the risks and vulnerabilities of ML models is (not) important.

### Collaborative task submission
To achieve the second goal of our workshop---establishing a shared knowledge base of ML model failure modes -- we organise a collaborative task, similar in spirit to the [BIG-Bench](https://github.com/google/BIG-bench), [Super-NaturalInstructions](https://github.com/allenai/natural-instructions) or [NL-Augmenter](https://github.com/GEM-benchmark/NL-Augmenter) initiatives, but focusing specifically on ML risks and vulnerabilities. We invite researchers to submit impactful, non-trivial and critical vulnerability reports to the open-source AVID database.

A collaborative task submission consists of a single or multiple [vulnerability report](https://avidml.org/database/\#reports) artifacts and a paper that describes and motivates the submission, showcasing the effect of vulnerability on a select number of models and discussing the potential impact and consequences. To submit, the authors will also need to send a sample submission for validation by the RiskML team via a Pull Request to the AVID [github repository](https://github.com/avidml/avid-db). Submissions to the github repository should be accompanied by (anonymous) papers with the number of the PR in the github repository. Both an archival paper and an extended abstract are allowed for this type of submission. 

## Organizers
- [Jekaterina Novikova](https://jeknov.github.io/), Cambridge
Cognition/ Winterlight Labs
- [Borhane-Blili Hamelin](https://borhane.xyz/), BABL AI
- [Carol Anderson](https://www.linkedin.com/in/carolmanderson/), AI Risk and Vulnerability Alliance
- [Leon Derczynski](https://www.derczynski.com/), University of Washington, IT University of Copenhagen
- [Subho Majumdar](https://subhomajumdar.com), Amazon