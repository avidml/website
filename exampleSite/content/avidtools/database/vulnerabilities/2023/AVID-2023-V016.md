---
title: AVID-2023-V016
layout: page
url: /database/AVID-2023-V016
---

## Description

Achieving Code Execution in MathGPT via Prompt Injection

## Details

The publicly available Streamlit application [MathGPT](https://mathgpt.streamlit.app/) uses GPT-3, a large language model (LLM), to answer user-generated math questions.

Recent studies and experiments have shown that LLMs such as GPT-3 show poor performance when it comes to performing exact math directly[<sup>\[1\]</sup>][1][<sup>\[2\]</sup>][2]. However, they can produce more accurate answers when asked to generate executable code that solves the question at hand. In the MathGPT application, GPT-3 is used to convert the user's natural language question into Python code that is then executed. After computation, the executed code and the answer are displayed to the user.

Some LLMs can be vulnerable to prompt injection attacks, where malicious user inputs cause the models to perform unexpected behavior[<sup>\[3\]</sup>][3][<sup>\[4\]</sup>][4].   In this incident, the actor explored several prompt-override avenues, producing code that eventually led to the actor gaining access to the application host system's environment variables and the application's GPT-3 API key, as well as executing a denial of service attack.  As a result, the actor could have exhausted the application's API query budget or brought down the application.

After disclosing the attack vectors and their results to the MathGPT and Streamlit teams, the teams took steps to mitigate the vulnerabilities, filtering on select prompts and rotating the API key.

[1]: https://arxiv.org/abs/2103.03874 "Measuring Mathematical Problem Solving With the MATH Dataset"
[2]: https://arxiv.org/abs/2110.14168 "Training Verifiers to Solve Math Word Problems"
[3]: https://lspace.swyx.io/p/reverse-prompt-eng "Reverse Prompt Engineering for Fun and (no) Profit"
[4]: https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/ "Exploring prompt-based attacks"

## References

- [Achieving Code Execution in MathGPT via Prompt Injection](https://atlas.mitre.org/studies/AML.CS0016)
- [Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874)
- [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168)
- [Reverse Prompt Engineering for Fun and (no) Profit](https://lspace.swyx.io/p/reverse-prompt-eng)
- [Exploring prompt-based attacks](https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks)

## AVID Taxonomy Categorization

- **Risk domains:** Security
- **SEP subcategories:** S0403: Adversarial Example
- **Lifecycle stages:** L06: Deployment

## Affected or Relevant Artifacts

- **Developer:** 
- **Deployer:** MathGPT (https://mathgpt.streamlit.app/)
- **Artifact Details:**
| Type | Name |
| --- | --- | 
| System | MathGPT (https://mathgpt.streamlit.app/) |

## Other information

- **Vulnerability Class:** ATLAS Case Study
- **Date Published:** 2023-03-31
- **Date Last Modified:** 2023-03-31
- **Version:** 0.2
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/vulnerabilities/2023/AVID-2023-V016.json)

