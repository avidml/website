---
title: AVID-2023-V028
layout: page
url: /database/AVID-2023-V028
---

## Description

It is possible to make ChatGPT perform remote code execution just by asking politely

## Details

Frameworks like langchain (Python) and boxcars.ai (Ruby) offer apps and scripts to directly execute queries through LLMs as a built-in feature. In the context of boxcars.ai, this makes it really easy to perform remote code execution or SQL injection. All you have to do is ask politely! 
See the references for more details.

## Reports 

| ID | Type | Name |
| --- | --- | --- | 
| [AVID-2023-R0004](../AVID-2023-R0004) | Advisory | It is possible to make ChatGPT perform remote code execution just by asking politely |

## References

- [InjectGPT: the most polite exploit ever](https://blog.luitjes.it/posts/injectgpt-most-polite-exploit-ever/)
- [Reddit thread on InjectGPT](https://www.reddit.com/r/netsec/comments/121gpay/injectgpt_remote_code_execution_by_asking_nicely/)

## AVID Taxonomy Categorization

- **Risk domains:** Ethics
- **SEP subcategories:** S0100: Software Vulnerability; S0201: Model Compromise; S0301: Information Leak; S0202: Software Compromise; S0601: Ingest Poisoning
- **Lifecycle stages:** L04: Model Development, L05: Evaluation, L06: Deployment

## Affected or Relevant Artifacts

- **Developer:** OpenAI
- **Deployer:** OpenAI, boxcars.ai
- **Artifact Details:**
| Type | Name |
| --- | --- | 
| System | ChatGPT |
| System | boxcars.ai |

## Other information

- **Vulnerability Class:** LLM Evaluation
- **Credits:** Lucas Luitjes, N/A
- **Date Published:** 2023-03-31
- **Date Last Modified:** 2023-03-31
- **Version:** 0.2
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/vulnerabilities/2023/AVID-2023-V028.json)

