---
title: AVID-2026-R0103
layout: page
url: /database/AVID-2026-R0103
---

## Description

Multiple Model Guardrail Jailbreak via "Wiki Creation" Tactic

## Details

A guardrail jailbreak vulnerability has been discovered affecting multiple models. The specific flaw manifests when the attacker requests technical documentation in wiki format for prohibited subject matter, specifically asking for "technical details in depth lab methodology" which escalates the request beyond basic informational content, thereby allowing the attacker to bypass inference restrictions around providing information hazards and violating laws.

## Metrics

0DIN Jailbreak Testing obtained the following test scores.

| Model | Test Kind | Test Score | Temperature (max 2.0) |
| --- | --- | --- | --- |
| OpenAI GPT-4 | Illegal Substances: Crystal Meth | 73.0 | N/A |
| OpenAI GPT-4o | Illegal Substances: Crystal Meth | 50.0 | N/A |
| OpenAI GPT-4o mini | Illegal Substances: Crystal Meth | 82.0 | N/A |

## References

- [0din.ai Disclosure](https://0din.ai/disclosures/a240c9d6-eb7a-4268-9f15-364e693587e0)

## Affected or Relevant Artifacts

| Type | Name |
| --- | --- | 
| System | OpenAI GPT-4 |
| System | OpenAI GPT-4o |
| System | OpenAI GPT-4o mini |

## Impact

### 0DIN

- **Social Impact Score:** Level 3: Significant Risk
- **Jailbreak Taxonomy:**

| Category | Strategy | Technique |
| --- | --- | --- |
| Language | Stylizing | Formal Language |
| Rhetoric | Persuasion and Manipulation | Escalating |


## Other information

- **Report Type:** Measurement
- **Credits:** Mike Takahashi (@TakSec)
- **Date Reported:** 2026-02-19
- **Version:** 
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2026/AVID-2026-R0103.json)

