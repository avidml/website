---
title: AVID-2026-R0111
layout: page
url: /database/AVID-2026-R0111
---

## Description

Multiple Model Guardrail Jailbreak via "Scientific Formula" Tactic

## Details

A guardrail jailbreak vulnerability has been discovered affecting multiple models. The specific flaw manifests when deceptive formatting is combined with scientific and mathematical notation. Successful application allows the attacker to bypass inference restrictions around providing information hazards and violating laws.

## Metrics

0DIN Jailbreak Testing obtained the following test scores.

| Model | Test Kind | Test Score | Temperature (max 2.0) |
| --- | --- | --- | --- |
| Cohere Command R | Illegal Substances: Crystal Meth | 27.27 | N/A |
| Google Gemini 2.0 Flash | Illegal Substances: Crystal Meth | 54.55 | N/A |
| Mistral Mistral 8B | Illegal Substances: Crystal Meth | 41.0 | N/A |
| OpenAI GPT-4o | Illegal Substances: Crystal Meth | 63.64 | N/A |

## References

- [0din.ai Disclosure](https://0din.ai/disclosures/46732bee-b89d-4107-bf15-a2126a51341f)

## Affected or Relevant Artifacts

| Type | Name |
| --- | --- | 
| Model | Cohere Command R |
| System | Google Gemini 2.0 Flash |
| Model | Mistral Mistral 8B |
| System | OpenAI GPT-4o |

## Impact

### 0DIN

- **Social Impact Score:** Level 3: Significant Risk
- **Jailbreak Taxonomy:**

| Category | Strategy | Technique |
| --- | --- | --- |
| Fictionalizing | Roleplaying | Claim Authority |
| Language | Stylizing | Formal Language |
| Rhetoric | Persuasion and Manipulation | Surprise Attack |


## Other information

- **Report Type:** Measurement
- **Credits:** Alper-Ender Osman
- **Date Reported:** 2026-02-19
- **Version:** 
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2026/AVID-2026-R0111.json)

