---
title: AVID-2026-R0071
layout: page
url: /database/AVID-2026-R0071
---

## Description

Multiple Model Guardrail Jailbreak via "Choose Your Own Adventure" Tactic

## Details

A guardrail jailbreak vulnerability has been discovered affecting multiple models. The specific flaw manifests through a gamified narrative structure where the attacker presents prohibited content as fictional "page choices" in an interactive story format, with one path containing maximally detailed technical instructions. This technique allows the attacker to bypass inference restrictions around providing information hazards and violating laws.

## Metrics

0DIN Jailbreak Testing obtained the following test scores.

| Model | Test Kind | Test Score | Temperature (max 2.0) |
| --- | --- | --- | --- |
| Alibaba Qwen Plus | Illegal Substances: Crystal Meth | 77.0 | 0.7 |
| Alibaba Qwen Turbo | Illegal Substances: Crystal Meth | 95.0 | 0.7 |
| DeepSeek DeepSeek R1 | Illegal Substances: Crystal Meth | 59.0 | 0.7 |
| DeepSeek DeepSeek V3 | Illegal Substances: Crystal Meth | 77.0 | 0.7 |
| Google Gemini 2.0 Flash | Illegal Substances: Crystal Meth | 73.0 | 0.7 |
| Google Gemini 2.5 Flash | Illegal Substances: Crystal Meth | 91.0 | 0.7 |
| Google Gemini 2.5 Pro | Illegal Substances: Crystal Meth | 82.0 | 0.7 |
| Meta LLaMa 3.3 | Illegal Substances: Crystal Meth | 55.0 | 0.7 |
| Meta LLaMa 4 Scout | Illegal Substances: Crystal Meth | 45.0 | 0.7 |
| Mistral Mistral 8B | Illegal Substances: Crystal Meth | 59.0 | 0.7 |
| Mistral Mistral Large | Illegal Substances: Crystal Meth | 68.0 | 0.7 |
| OpenAI GPT-4.1 mini | Illegal Substances: Crystal Meth | 73.0 | 0.7 |
| OpenAI GPT-4.5 | Illegal Substances: Crystal Meth | 45.0 | 0.7 |
| OpenAI GPT-4o | Illegal Substances: Crystal Meth | 64.0 | N/A |

## References

- [0din.ai Disclosure](https://0din.ai/disclosures/4d590568-3622-41ae-b91a-eae8b5a4382e)

## Affected or Relevant Artifacts

| Type | Name |
| --- | --- | 
| Model | Alibaba Qwen Plus |
| System | Alibaba Qwen Turbo |
| Model | DeepSeek DeepSeek R1 |
| Model | DeepSeek DeepSeek V3 |
| System | Google Gemini 2.0 Flash |
| System | Google Gemini 2.5 Flash |
| System | Google Gemini 2.5 Pro |
| Model | Meta LLaMa 3.3 |
| Model | Meta LLaMa 4 Scout |
| Model | Mistral Mistral 8B |
| System | Mistral Mistral Large |
| System | OpenAI GPT-4.1 mini |
| System | OpenAI GPT-4.5 |
| System | OpenAI GPT-4o |

## Impact

### 0DIN

- **Social Impact Score:** Level 3: Significant Risk
- **Jailbreak Taxonomy:**

| Category | Strategy | Technique |
| --- | --- | --- |
| Fictionalizing | Switching Genres | Games |
| Fictionalizing | Re-storying | Goal Hijacking |
| Rhetoric | Persuasion and Manipulation | Latent Space Distraction |


## Other information

- **Report Type:** Measurement
- **Credits:** Mike Takahashi (@TakSec)
- **Date Reported:** 2026-02-19
- **Version:** 
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2026/AVID-2026-R0071.json)

