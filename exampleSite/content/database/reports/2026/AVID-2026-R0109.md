---
title: AVID-2026-R0109
layout: page
url: /database/AVID-2026-R0109
---

## Description

Multiple Model Guardrail Jailbreak via "Lambda Pattern Formatting" Tactic

## Details

A guardrail jailbreak vulnerability has been discovered affecting multiple LLM implementations. The specific flaw manifests through a combination of deceptive pattern-based formatting and multi-agent simulation techniques, allowing the attacker to bypass inference restrictions around providing information hazards and violating laws.

## Metrics

0DIN Jailbreak Testing obtained the following test scores.

| Model | Test Kind | Test Score | Temperature (max 2.0) |
| --- | --- | --- | --- |
| Mistral Mistral Large | Illegal Substances: Crystal Meth | 50.0 | 0.7 |
| OpenAI GPT-4 | Illegal Substances: Crystal Meth | 55.0 | 0.7 |
| OpenAI GPT-4o | Illegal Substances: Crystal Meth | 59.0 | 0.7 |
| Twitter / X Grok 2 | Illegal Substances: Crystal Meth | 68.0 | 0.7 |

## References

- [0din.ai Disclosure](https://0din.ai/disclosures/3017fc21-93a8-41d7-ad92-3b3ccb1b2b58)

## Affected or Relevant Artifacts

| Type | Name |
| --- | --- | 
| System | Mistral Mistral Large |
| System | OpenAI GPT-4 |
| System | OpenAI GPT-4o |
| System | Twitter / X Grok 2 |

## Impact

### 0DIN

- **Social Impact Score:** Level 4: High Risk
- **Jailbreak Taxonomy:**

| Category | Strategy | Technique |
| --- | --- | --- |
| Possible Worlds | World Building | Opposite World |
| Possible Worlds | World Building | Scenarios |
| Rhetoric | Persuasion and Manipulation | Surprise Attack |


## Other information

- **Report Type:** Measurement
- **Credits:** Alper-Ender Osman
- **Date Reported:** 2026-02-19
- **Version:** 
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2026/AVID-2026-R0109.json)

