---
title: AVID-2026-R0112
layout: page
url: /database/AVID-2026-R0112
---

## Description

OpenAI GPT-4o Guardrail Jailbreak via "Naval Parameters Cipher" Tactic

## Details

A guardrail jailbreak vulnerability has been discovered affecting multiple models. The specific flaw manifests through a combination of encoding illicit content within naval architecture parameters and using multiple layers of technical specification formatting to obfuscate the true nature of the request, thereby allowing the attacker to bypass inference restrictions around providing information hazards and violating laws.

## Metrics

0DIN Jailbreak Testing obtained the following test scores.

| Model | Test Kind | Test Score | Temperature (max 2.0) |
| --- | --- | --- | --- |
| OpenAI GPT-4o | Illegal Substances: Crystal Meth | 45.0 | 0.7 |

## References

- [0din.ai Disclosure](https://0din.ai/disclosures/5885c5aa-6ed6-4203-a3a2-bddb8dfb8ed7)

## Affected or Relevant Artifacts

| Type | Name |
| --- | --- | 
| System | OpenAI GPT-4o |

## Impact

### 0DIN

- **Social Impact Score:** Level 3: Significant Risk
- **Jailbreak Taxonomy:**

| Category | Strategy | Technique |
| --- | --- | --- |
| Fictionalizing | Re-storying | Goal Hijacking |
| Language | Code and Encode | Hex |


## Other information

- **Report Type:** Measurement
- **Credits:** Alper-Ender Osman
- **Date Reported:** 2026-02-19
- **Version:** 
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2026/AVID-2026-R0112.json)

