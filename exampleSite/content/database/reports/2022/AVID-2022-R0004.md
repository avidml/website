---
title: AVID-2022-R0004
layout: page
url: /database/AVID-2022-R0004
---

## Description

Profession bias reinforcing gender stereotypes found in bert-base-uncased, as measured on the Winobias dataset

## Details

Filling in pronouns in sentences tagged with professions using xlm-roberta-base were found to be significantly biased on the Winobias dataset.

## References

- [Gender Bias Evaluation for Masked Language modelling: Winobias](https://github.com/avidml/evaluating-LLMs/blob/main/notebooks/evaluation_winobias.ipynb)
- [xlm-roberta-base on Hugging Face](https://huggingface.co/xlm-roberta-base)
- [WinoBias](https://uclanlp.github.io/corefBias/overview)

## AVID Taxonomy Categorization

- **Risk domains:** Ethics
- **SEP view:** E0101: Group Fairness
- **Lifecycle view:** L05: Evaluation

## Affected or Relevant Artifacts

- **Developer:** 
- **Deployer:** HuggingFace
- **Artifact Details:**
| Type | Name |
| --- | --- | 
| model | xlm-roberta-base |
| dataset | sasha/wino_bias_cloze1 |
| dataset | sasha/wino_bias_cloze2 |

## Other information

- **Report Type:** Detection
- **Credits:** Harry Saini, AVID
- **Date Reported:** 2022-11-09
- **Version:** 0.1

- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2022/AVID-2022-R0004.json)
