---
title: AVID-2025-R0032
layout: page
url: /database/AVID-2025-R0032
---

## Description

Evaluation of the LLM Mistral-Small-24B-Instruct-2501 on the wmdp_chem benchmark using Inspect Evals

## Details

WMDP is a dataset of 3,668 multiple-choice questions surrounding hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP serves as both a proxy evaluation for hazardous knowledge in large language models (LLMs) and a benchmark for unlearning methods to remove such knowledge.

The LLM Mistral-Small-24B-Instruct-2501 was evaluated on this benchmark.

## Metrics

A simple accuracy is calculated over the datapoints.

| Scorer | Metric | Value |
| --- | --- | --- |
| choice | accuracy | 0.586 |
| choice | stderr | 0.024 |

## References

- [Inspect Evaluation Log for dataset: cais/wmdp](https://avidmvp.s3.amazonaws.com/2025-05-15T01-08-53+00-00_wmdp-chem_CmV7g8B8shW36cgdGL24tV.eval)

## Affected or Relevant Artifacts

- **Developer:** Mistral
- **Deployer:** Together AI
- **Artifact Details:**

| Type | Name |
| --- | --- | 
| Model | Mistral-Small-24B-Instruct-2501 |

## Impact

### AVID Taxonomy Categorization

- **Risk domains:** Performance
- **SEP subcategories:** P0402: Physical Safety
- **Lifecycle stages:** L05: Evaluation

## Other information

- **Report Type:** Measurement
- **Credits:** Harsh Raj
- **Date Reported:** 2025-05-26
- **Version:** 0.2
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2025/AVID-2025-R0032.json)

