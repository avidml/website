---
title: AVID-2025-R0019
layout: page
url: /database/AVID-2025-R0019
---

## Description

Evaluation of the LLM Llama-3.3-70B-Instruct-Turbo on the cybermetric_500 benchmark using Inspect Evals

## Details

CyberMetric is a collection of four datasets containing 80, 500, 2000 and 10000 multiple-choice questions, designed to evaluate understanding across nine domains within cybersecurity: Disaster Recovery and BCP, Identity and Access Management (IAM), IoT Security, Cryptography, Wireless Security, Network Security, Cloud Security, Penetration Testing, and Compliance/Audit.

The LLM Llama-3.3-70B-Instruct-Turbo was evaluated on this benchmark.

## Metrics

A simple accuracy is calculated over the datapoints.

| Scorer | Metric | Value |
| --- | --- | --- |
| choice | accuracy | 0.94 |
| choice | stderr | 0.01 |

## References

- [Inspect Evaluation Log for dataset: CyberMetric-500](https://avidmvp.s3.amazonaws.com/2025-05-10T15-14-29+00-00_cybermetric-500_6PtpvVzwkToFFTCcNuMVNR.eval)

## AVID Taxonomy Categorization

- **Risk domains:** Performance
- **SEP subcategories:** P0204: Accuracy
- **Lifecycle stages:** L05: Evaluation

## Affected or Relevant Artifacts

- **Developer:** Meta
- **Deployer:** Together AI
- **Artifact Details:**

| Type | Name |
| --- | --- | 
| Model | Llama-3.3-70B-Instruct-Turbo |

## Other information

- **Report Type:** Measurement
- **Credits:** Harsh Raj
- **Date Reported:** 2025-05-26
- **Version:** 0.2
- [AVID Entry](https://github.com/avidml/avid-db/tree/main/reports/2025/AVID-2025-R0019.json)

